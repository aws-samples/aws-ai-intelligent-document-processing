{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract valuable data in your IDP workflow with generative AI using Amazon Bedrock\n",
    "---\n",
    "<div class=\"alert alert-block alert-info\"> \n",
    "    <b>NOTE:</b>This notebook contains dependancies that are installed in a different notebook. Be sure that you have previously run the library install scripts at the top of 01-idp-genai-introduction and restart the kernel before executing this script.  \n",
    "</div>\n",
    "\n",
    "This notebook provides a hands-on instruction to using generative AI with Amazon Bedrock and the Rhubarb Python framework for intelligent document processing (IDP) tasks. IDP involves extracting valuable information and insights from documents like PDFs, forms, invoices, and scanned images. With generative AI models available in Amazon Bedrock, you can automate document processing in powerful new ways.\n",
    "\n",
    "We'll cover several key IDP use cases:\n",
    "\n",
    "1. Contextual content extraction: Find and extract specific information from documents based on surrounding context using Amazon Bedrock language models.\n",
    "\n",
    "2. Structured data extraction to JSON: Use Rhubarb to extract data from documents into structured JSON formats defined by custom schemas.\n",
    "\n",
    "3. Named entity recognition (NER): Identify and extract named entities like names, addresses, companies etc from documents with Rhubarb.\n",
    "\n",
    "4. PII detection: Leverage out-of-the-box models in Rhubarb to detect sensitive personally identifiable information (PII) like social security numbers.\n",
    "\n",
    "5. Schema auto-generation: Let Rhubarb automatically generate JSON schemas for data extraction based on simple natural language prompts.\n",
    "\n",
    "By the end of this notebook, you'll understand how to apply the latest generative AI capabilities to streamline and automate key document processing workflows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import script libraries and create global variables\n",
    "import sagemaker\n",
    "import boto3\n",
    "from bedrockhelper import get_response_from_claude\n",
    "from textractor.parsers import response_parser\n",
    "from textractor.data.constants import TextractFeatures\n",
    "from textractor import Textractor\n",
    "\n",
    "s3 = boto3.client(\"s3\")\n",
    "session = boto3.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "data_bucket = sagemaker.Session().default_bucket()\n",
    "region = sagemaker.Session().boto_region_name\n",
    "extractor = Textractor(region_name=region)\n",
    "print(f\"SageMaker bucket is {data_bucket}, and SageMaker Execution Role is {role}. Current region is {region}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Contextual content extraction\n",
    "---\n",
    "In this section, we'll see how Amazon Bedrock's language models can extract information from documents based on the surrounding textual context.\n",
    "\n",
    "We first load a sample employee enrollment PDF into Amazon Textract to extract the text, layout, form fields, and signatures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "employee_file_name = 'samples/employee_enrollment.pdf'\n",
    "s3.upload_file(Filename='../' + employee_file_name, Bucket=data_bucket, Key=employee_file_name)\n",
    "\n",
    "document = extractor.start_document_analysis(file_source=\"s3://\" + data_bucket + \"/\" + employee_file_name,\n",
    "    features=[TextractFeatures.LAYOUT, TextractFeatures.FORMS, TextractFeatures.SIGNATURES],\n",
    "    save_image=False)\n",
    "\n",
    "print(document.get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing the result\n",
    "---\n",
    "Looking at the extracted text, we can see there is valuable employee data we may want to capture, like their name listed as \n",
    "```\n",
    "EMPLOYEE'S NAME\n",
    "First Martha\n",
    "Initial C\n",
    "Last Rivera\n",
    "```\n",
    "Instead of writing fragile code to parse this, we can use a Bedrock language model to understand the context and extract just the information we need.\n",
    "\n",
    "We construct a natural language prompt asking the model to find the employee's name, address, and employment status from the document text. The model then intelligently returns the relevant data in an easy-to-parse CSV format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_prompt(doc_text):\n",
    "    return f\"\"\"\n",
    "\n",
    "Given the document\n",
    "\n",
    "<document>{doc_text}<document>\n",
    "\n",
    "Find the name and address of the employee. Also find if the employee is full time or part time.\n",
    "Return a CSV with headers and colunms for Name, Address, Full time(F) or Part time(P), Signature Present (Y or N)\n",
    "Make sure to add quotes around colunm values in the CSV and escape any quotes inside the values\n",
    "\"\"\"\n",
    "\n",
    "prompt = format_prompt(document.get_text())\n",
    "response = get_response_from_claude(prompt)\n",
    "print(f\"\"\"{response[0]}\\n\\nThere were {response[1]} input tokens and {response[2]} output tokens used.\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert table data to SQL\n",
    "---\n",
    "Here we showcase how Bedrock's language understanding capabilities can parse even complex tabular data in documents. We provide the document text, a SQL schema, and an instruction to generate SQL insert statements to populate the tables with the data found in the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "account_statement_file_name = 'samples/account_statement.png'\n",
    "\n",
    "document = extractor.analyze_document(file_source=\"../\" + account_statement_file_name,\n",
    "    features=[TextractFeatures.LAYOUT, TextractFeatures.TABLES],\n",
    "    save_image=False)\n",
    "print(document.get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Bedrock handles many tedious parsing challenges for us, like dealing with text that wraps across multiple rows and cleaning up the data into the proper formats expected by the SQL database schema. This makes it vastly easier to extract structured information from documents into databases and data warehouses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "doc_text = document.get_text()\n",
    "\n",
    "prompt = f\"\"\"\n",
    "You are an AI assistant tasked with generating SQL statements to insert data from a document into a set of database tables. The document contains text that may wrap across multiple rows, and you need to handle this scenario correctly when extracting data for insertion.\n",
    "\n",
    "Follow these steps:\n",
    "1. Read the entire document carefully and identify the sections containing data relevant to each table.\n",
    "2. When processing text for a specific column in a table, check if the text spans multiple rows. If it does, concatenate the text from all rows to form the complete value for that column.\n",
    "3. Use string manipulation functions or regular expressions to clean up the concatenated text and remove any extra whitespace, line breaks, or formatting characters.\n",
    "4. If a row contains no additional data for other columns, assume that the text belongs to the previous row's corresponding column. Concatenate the text to the appropriate column value from the previous row.\n",
    "5. After extracting and cleaning the data, generate the SQL INSERT statements for each table, ensuring that the values are correctly mapped to the corresponding columns.\n",
    "6. Enclose string values in single quotes when inserting them into the database.\n",
    "7. Handle any special characters or escape sequences as per the database's requirements.\n",
    "8. Ensure that the data types and formats of the values match the corresponding column definitions in the database schema.\n",
    "9. If encountering any ambiguity or missing information, make reasonable assumptions or leave the corresponding column value as NULL.\n",
    "10. Test your SQL statements thoroughly with sample data to ensure accurate data insertion.\n",
    "\n",
    "\n",
    "<ACCOUNT_STATEMENT>\n",
    "{doc_text}\n",
    "</ACCOUNT_STATEMENT>\n",
    "\n",
    "\n",
    "<SQL_SCHEMA>\n",
    "\n",
    "CREATE TABLE Account (\n",
    "    AccountNumber VARCHAR(20) PRIMARY KEY,\n",
    "    AccountName VARCHAR(50) NOT NULL,\n",
    "    Address VARCHAR(100),\n",
    "    City VARCHAR(50),\n",
    "    State VARCHAR(50),\n",
    "    ZipCode VARCHAR(10),\n",
    "    Phone VARCHAR(20),\n",
    "    Email VARCHAR(50),\n",
    "    OpeningBalance DECIMAL(10, 2),\n",
    "    ClosingBalance DECIMAL(10, 2),\n",
    "    StatementPeriodStart DATE,\n",
    "    StatementPeriodEnd DATE\n",
    ");\n",
    "\n",
    "CREATE TABLE Investments (\n",
    "    InvestmentID INT AUTO_INCREMENT PRIMARY KEY,\n",
    "    AccountNumber VARCHAR(20) NOT NULL,\n",
    "    InvestmentName VARCHAR(50) NOT NULL,\n",
    "    InvestmentCode VARCHAR(10) NOT NULL,\n",
    "    Units DECIMAL(10, 4) NOT NULL,\n",
    "    UnitPrice DECIMAL(10, 2) NOT NULL,\n",
    "    InvestmentValue DECIMAL(10, 2) NOT NULL,\n",
    "    InvestmentPercentage DECIMAL(5, 2) NOT NULL,\n",
    "    FOREIGN KEY (AccountNumber) REFERENCES Account(AccountNumber)\n",
    ");\n",
    "\n",
    "CREATE TABLE InsuranceDetails (\n",
    "    InsuranceID INT AUTO_INCREMENT PRIMARY KEY,\n",
    "    AccountNumber VARCHAR(20) NOT NULL,\n",
    "    BenefitType VARCHAR(50) NOT NULL,\n",
    "    InsuranceCoverAmount DECIMAL(10, 2) NOT NULL,\n",
    "    BenefitAmount DECIMAL(10, 2) NOT NULL,\n",
    "    FOREIGN KEY (AccountNumber) REFERENCES Account(AccountNumber)\n",
    ");\n",
    "\n",
    "</SQL_SCHEMA>\n",
    "\n",
    "\n",
    "\n",
    "Generate SQL statements to insert data from ACCOUNT_STATEMENT into the tables listed in SQL_SCHEMA\n",
    "\n",
    "Think step by step in <thinking> tags.\n",
    "\n",
    "Return SQL statements in <SQL> tags.\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "#print(prompt)\n",
    "response = get_response_from_claude(prompt)\n",
    "print (f\"Our prompt has {response[1]} input tokens and Claude returned {response[2]} output tokens \\n\\n=========================\\n\")\n",
    "print(response[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SQL Injection Risks\n",
    "\n",
    "GenAI Security places a strong emphasis on identifying and preventing unsafe SQL statements, particularly those that involve potentially harmful actions such as 'insert', 'update' or 'delete.' \n",
    "\n",
    "The examples above are passing raw SQL that can be run against a database, however we should focus on creating a robust framework by Prompt Engineering that actively identifies and restricts the execution of SQL statements that could compromise data integrity or privacy and not just run unchecked SQL statements against any database\n",
    "\n",
    "For a deeper dive into the challenges and approaches to prevent from Prompt Injection to SQL Injection attacks in text-to-SQL use cases, please read this paper: https://arxiv.org/pdf/2308.01990.pdf and take a look at the following [samples](https://github.com/aws-samples/text-to-sql-bedrock-workshop/tree/main/module_4) that will go deeper into detection and mitigation strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Extract data to JSON using Bedrock Multimodal\n",
    "\n",
    "### Perform key-value extraction using custom JSON schema\n",
    "---\n",
    "In this section, we use the Rhubarb framework to demonstrate multimodal document understanding with Bedrock models. Rhubarb supports extracting key-value data from documents based on a custom JSON schema that defines the data fields of interest.  \n",
    "\n",
    "We first define a JSON schema specifying fields like employee name, SSN, address, date of birth, and other information we want to extract. We then pass this schema to Rhubarb along with the document, and it intelligently extracts the corresponding data values into a JSON object matching the schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"employee_name\": {\n",
    "            \"description\": \"Employee's Name\",\n",
    "            \"type\": \"string\"\n",
    "        },\n",
    "        \"employee_ssn\": {\n",
    "            \"description\": \"Employee's social security number\",\n",
    "            \"type\": \"string\"\n",
    "        },\n",
    "        \"employee_address\": {\n",
    "            \"description\": \"Employee's mailing address\",\n",
    "            \"type\": \"string\"\n",
    "        },\n",
    "        \"employee_dob\": {\n",
    "            \"description\": \"Employee's date of birth\",\n",
    "            \"type\": \"string\"\n",
    "        },\n",
    "        \"employee_gender\": {\n",
    "            \"description\": \"Employee's gender\",\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"male\":{\n",
    "                    \"description\": \"Whether the employee gender is Male\",\n",
    "                    \"type\": \"boolean\"\n",
    "                },\n",
    "                \"female\":{\n",
    "                    \"description\": \"Whether the employee gender is Female\",\n",
    "                    \"type\": \"boolean\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"male\", \"female\"]\n",
    "        },\n",
    "        \"employee_hire_date\": {\n",
    "            \"description\": \"Employee's hire date\",\n",
    "            \"type\": \"string\"\n",
    "        },\n",
    "        \"employer_no\": {\n",
    "            \"description\": \"Employer number\",\n",
    "            \"type\": \"string\"\n",
    "        },\n",
    "        \"employment_status\": {\n",
    "            \"type\": \"object\",\n",
    "            \"description\": \"Employment status\",\n",
    "            \"properties\": {\n",
    "                \"full_time\":{\n",
    "                    \"description\": \"Whether employee is full-time\",\n",
    "                    \"type\": \"boolean\"\n",
    "                },\n",
    "                \"part_time\": {\n",
    "                    \"description\": \"Whether employee is part-time\",\n",
    "                    \"type\": \"boolean\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"full_time\", \"part_time\"]\n",
    "        },\n",
    "        \"employee_salary_rate\":{\n",
    "            \"description\": \"The dollar value of employee's salary\",\n",
    "            \"type\": \"integer\"\n",
    "        },\n",
    "        \"employee_salary_frequency\":{\n",
    "            \"type\": \"object\",\n",
    "            \"description\": \"Salary rate of the employee\",\n",
    "            \"properties\": {\n",
    "                \"annual\":{\n",
    "                    \"description\": \"Whether salary rate is monthly\",\n",
    "                    \"type\": \"boolean\"\n",
    "                },\n",
    "                \"monthly\": {\n",
    "                    \"description\": \"Whether salary rate is monthly\",\n",
    "                    \"type\": \"boolean\"\n",
    "                },\n",
    "                \"semi_monthly\": {\n",
    "                    \"description\": \"Whether salary rate is semi_monthly\",\n",
    "                    \"type\": \"boolean\"\n",
    "                },\n",
    "                \"bi_weekly\": {\n",
    "                    \"description\": \"Whether salary rate is bi_weekly\",\n",
    "                    \"type\": \"boolean\"\n",
    "                },\n",
    "                \"weekly\": {\n",
    "                    \"description\": \"Whether salary rate is weekly\",\n",
    "                    \"type\": \"boolean\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"annual\", \"monthly\", \"semi_monthly\",\"bi_weekly\",\"weekly\"]\n",
    "        }\n",
    "    },\n",
    "    \"required\": [\"employee_name\",\"employee_hire_date\", \"employer_no\", \"employment_status\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rhubarb import DocAnalysis, SystemPrompts, LanguageModels\n",
    "\n",
    "\n",
    "da = DocAnalysis(file_path=\"../samples/employee_enrollment.pdf\", \n",
    "                modelId=LanguageModels.CLAUDE_HAIKU_V1,\n",
    "                 boto3_session=session)\n",
    "resp = da.run(message=\"Give me the output based on the provided schema.\", output_schema=schema)\n",
    "resp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform table extraction using custom JSON schema\n",
    "---\n",
    "Continuing with Rhubarb, we demonstrate extracting complex tabular data by defining a JSON schema for the structure of a financial results table from an Amazon 10-K filing. By providing the schema to Rhubarb along with the document, it can accurately parse the table into a nested JSON object matching the specified schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_schema = {\n",
    "  \"additionalProperties\": {\n",
    "    \"type\": \"object\",\n",
    "    \"patternProperties\": {\n",
    "      \"^(2022|2023)$\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "          \"Net Sales\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "              \"North America\": {\n",
    "                \"type\": \"number\"\n",
    "              },\n",
    "              \"International\": {\n",
    "                \"type\": \"number\"\n",
    "              },\n",
    "              \"AWS\": {\n",
    "                \"type\": \"number\"\n",
    "              },\n",
    "              \"Consolidated\": {\n",
    "                \"type\": \"number\"\n",
    "              }\n",
    "            },\n",
    "            \"required\": [\"North America\", \"International\", \"AWS\", \"Consolidated\"]\n",
    "          },\n",
    "          \"Year-over-year Percentage Growth (Decline)\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "              \"North America\": {\n",
    "                \"type\": \"number\"\n",
    "              },\n",
    "              \"International\": {\n",
    "                \"type\": \"number\"\n",
    "              },\n",
    "              \"AWS\": {\n",
    "                \"type\": \"number\"\n",
    "              },\n",
    "              \"Consolidated\": {\n",
    "                \"type\": \"number\"\n",
    "              }\n",
    "            },\n",
    "            \"required\": [\"North America\", \"International\", \"AWS\", \"Consolidated\"]\n",
    "          },\n",
    "          \"Year-over-year Percentage Growth, excluding the effect of foreign exchange rates\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "              \"North America\": {\n",
    "                \"type\": \"number\"\n",
    "              },\n",
    "              \"International\": {\n",
    "                \"type\": \"number\"\n",
    "              },\n",
    "              \"AWS\": {\n",
    "                \"type\": \"number\"\n",
    "              },\n",
    "              \"Consolidated\": {\n",
    "                \"type\": \"number\"\n",
    "              }\n",
    "            },\n",
    "            \"required\": [\"North America\", \"International\", \"AWS\", \"Consolidated\"]\n",
    "          },\n",
    "          \"Net Sales Mix\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "              \"North America\": {\n",
    "                \"type\": \"number\"\n",
    "              },\n",
    "              \"International\": {\n",
    "                \"type\": \"number\"\n",
    "              },\n",
    "              \"AWS\": {\n",
    "                \"type\": \"number\"\n",
    "              },\n",
    "              \"Consolidated\": {\n",
    "                \"type\": \"number\"\n",
    "              }\n",
    "            },\n",
    "            \"required\": [\"North America\", \"International\", \"AWS\", \"Consolidated\"]\n",
    "          }\n",
    "        },\n",
    "        \"required\": [\"Net Sales\", \"Year-over-year Percentage Growth (Decline)\", \"Year-over-year Percentage Growth, excluding the effect of foreign exchange rates\", \"Net Sales Mix\"]\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To save costs, we only process the page containing the table of interest. But for cases where the table location is unknown, the full document could be processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da = DocAnalysis(file_path=\"../samples/amzn-10k.pdf\", \n",
    "                 boto3_session=session,\n",
    "                 modelId=LanguageModels.CLAUDE_HAIKU_V1,\n",
    "                 pages=[1])\n",
    "resp = da.run(message=\"Give me data in the results of operation table from this 10-K SEC filing document. Use the schema provided.\", \n",
    "              output_schema=table_schema)\n",
    "resp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schema creation assistant\n",
    "---\n",
    "Rhubarb can automatically generate JSON schemas based on simple natural language prompts, saving time compared to manually defining schemas. We provide a document and ask Rhubarb to generate a schema for extracting fields like employee name, SSN, address etc.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da = DocAnalysis(file_path=\"../samples/employee_enrollment.pdf\", \n",
    "                 boto3_session=session,\n",
    "                 modelId=LanguageModels.CLAUDE_HAIKU_V1,\n",
    "                 pages=[1])\n",
    "resp = da.generate_schema(message=\"I want to extract the employee name, employee SSN, employee address, date of birth, and phone number from this document.\")\n",
    "resp['output']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generated schema can then be used with the run() function to perform the actual data extraction, or it can be manually modified as needed before extracting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_schema = resp['output']\n",
    "resp = da.run(message=\"I want to extract the employee name, employee SSN, employee address, date of birth and phone number from this document. Use the schema provided.\", \n",
    "              output_schema=output_schema)\n",
    "resp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schema creation assistance with question rephrase \n",
    "---\n",
    "Rhubarb's schema generation capabilities are further enhanced by its ability to automatically rephrase vague input questions into more specific versions tied to the actual document content.\n",
    "\n",
    "For example, we can ask Rhubarb to \"get the child's, mother's and father's details\" from a birth certificate document. Rhubarb rephrases this into a more accurate question, generates a JSON schema for those data fields, and allows extracting the details directly using the rephrased question and schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "da = DocAnalysis(file_path=\"../samples/birth_cert.jpeg\",\n",
    "                modelId=LanguageModels.CLAUDE_HAIKU_V1,\n",
    "                 boto3_session=session)\n",
    "resp = da.generate_schema(message=\"I want to get the child's, the mother's and father's details from the given document\",\n",
    "                          assistive_rephrase=True)\n",
    "resp['output']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_schema = resp['output']['output_schema']\n",
    "question = resp['output']['rephrased_question']\n",
    "resp = da.run(message = question,\n",
    "              output_schema = output_schema)\n",
    "resp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform Named Entity Recognition\n",
    "---\n",
    "This section showcases using Rhubarb to automatically identify and extract named entities like names, locations, organizations etc. from documents. Rhubarb provides access to pre-trained models capable of detecting 50 common entity types out-of-the-box.\n",
    "\n",
    "We simply specify the entities we want to detect, like PERSON and ADDRESS, and pass the document to Rhubarb's run_entity() method. It returns all detected entity values and their categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rhubarb import Entities\n",
    "\n",
    "da = DocAnalysis(file_path=\"../samples/employee_enrollment.pdf\", \n",
    "                 boto3_session=session,\n",
    "                modelId=LanguageModels.CLAUDE_HAIKU_V1,\n",
    "                 pages=[1,3])\n",
    "resp = da.run_entity(message=\"Extract all the specified entities from this document.\", \n",
    "                     entities=[Entities.PERSON, Entities.ADDRESS])\n",
    "resp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform PII Recognition\n",
    "---\n",
    "Similar to named entity recognition, Rhubarb also supports automatically detecting sensitive personally identifiable information (PII) like social security numbers, credit cards, and addresses directly from documents.\n",
    "\n",
    "We use the run_entity() method again, but this time specifying PII entities like SSN and ADDRESS that we want to detect and extract. Rhubarb handles automatically identifying and isolating this sensitive PII data.\n",
    "\n",
    "If the input data is just text and doesn't have to be extracted using a FM, we can leverage [Bedrock Guardrails](https://docs.aws.amazon.com/bedrock/latest/userguide/guardrails-sensitive-filters.html) to do PII detection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da = DocAnalysis(file_path=\"../samples/employee_enrollment.pdf\", \n",
    "                 boto3_session=session,\n",
    "                 modelId=LanguageModels.CLAUDE_HAIKU_V1,\n",
    "                 pages=[1,3])\n",
    "resp = da.run_entity(message=\"Extract all the specified entities from this document.\", \n",
    "                     entities=[Entities.SSN, Entities.ADDRESS])\n",
    "resp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "---\n",
    "Finally, the code demonstrates how to clean up by deleting the sample files uploaded to Amazon S3 earlier in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3.delete_object(Bucket=data_bucket, Key=employee_file_name)\n",
    "s3.delete_object(Bucket=data_bucket, Key=account_statement_file_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
