{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Bedrocks Titan Multi-Modal embeddings to create a gallery of document images for classification.\n",
    "---\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"> \n",
    "    <b>NOTE:</b> For SageMaker <b>studio classic</b> will need to use a Jupyter Kernel with Python 3.9 or above to use this notebook. If you are in Amazon SageMaker Studio, you can use the `SageMaker Distribution 1.4` image.\n",
    "    You can ignore any ERROR or WARNINGS during the `pip installs`.\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\"> \n",
    "    <b>NOTE:</b> You will need model access to <b>Titan Multimodal Embeddings Generation 1 (G1)</b> to be able to run this notebook. Verify if you have access to the model by going to <a href=\"https://console.aws.amazon.com/bedrock\" target=\"_blank\">Amazon Bedrock console</a> > left menu \"Model access\". The \"Access status\" for Titan Multimodal Embeddings G1 must be in \"Access granted\" status in green. If you do not have access, then click \"Edit\" button on the top right > select the model checkbox > click \"Save changes\" button at the bottom. You should have access to the model within a few moments.\n",
    "</div>\n",
    "\n",
    "In this notebook we will show how you can use Titan multimodal model to create embeddings of various document types from their images. We will then store these embeddings in an in-memory vector database that will be used as our gallery. \n",
    "\n",
    "Next, we will take some random sample of similar documents, create embeddings of these and do a similarity search against our in-memory vector database to find the closest single match. We will then use this matched known document in our gallery to help identify and classify the randomly selected document. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets start by installing our dependencies. Boto3 along with Botocore provide our python AWS SDK libary for the API calls. FAISS is a open source Vector DB we will use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q -U botocore boto3 faiss-cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this notebook demonstration we will copy some sample documents consisting of Bank Statements, Closing Disclosures, Invoices, Social Security cards and W4's into a local folder that we will use to create our embeddings from and populate into the Vector DB that we will use as a gallery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy zip containing sample files from a S3 location to our local directory\n",
    "!curl https://idp-assets-wwso.s3.us-east-2.amazonaws.com/workshop-data/docClassificationSamples.zip --output docClassificationSamples.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unpack Zip file containing our sample and testing documents to a local directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "EXTRACTDIR: str = \"classification-embedding-samples\"\n",
    "\n",
    "shutil.unpack_archive(\"./docClassificationSamples.zip\", extract_dir=EXTRACTDIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Vector database\n",
    "Now we will create our in-memory vector database. For this demonstration we will use open source FAISS. Faiss is a library for efficient similarity search and clustering of dense vectors. See https://github.com/facebookresearch/faiss/wiki\n",
    "We will first check to see if the Vector DB has already been created and saved by reading first from disk, otherwise we will create a new instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import faiss\n",
    "\n",
    "INDEX_NAME: str = \"faissGallery.index\"  # Name of our vector DB index\n",
    "\n",
    "# check to see if FAISS index is written to disk \n",
    "# from previous run and load into memory\n",
    "if os.path.isfile(INDEX_NAME):\n",
    "    indexIDMap=faiss.read_index(INDEX_NAME)\n",
    "else:\n",
    "    index = faiss.IndexFlatL2(1024)\n",
    "    indexIDMap = faiss.IndexIDMap(index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Create embeddings from images and store\n",
    "To generate our embeddings from the document known image types we just copied, we will utilize Amazon Bedrock. Bedrock is a fully managed service that makes foundation models from leading AI startups and Amazon available via an API. For our purposes here we will utilize Amazon Titan Multimodal Embeddings Generation 1 (G1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "bedrock = boto3.client(\n",
    "     service_name='bedrock-runtime',\n",
    "     region_name='us-west-2'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will create a function that we will call repeatably for each document in our sample that will fetch embeddings from Titan. While Titan multimodal embeddings Generation 1 is multimodal, meaning we can create embeddings for both text and or image combined, for this demonstration we will only be creating embeddings for image that we will use in our vector DB to generate the gallery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, numpy as np\n",
    "\n",
    "\n",
    "def getEmbeddings(inputImageB64):\n",
    "    request_body = {}\n",
    "    request_body[\"inputText\"] = None  # not using any text\n",
    "    request_body[\"inputImage\"] = inputImageB64\n",
    "    body = json.dumps(request_body)\n",
    "    response = bedrock.invoke_model(\n",
    "        body=body,\n",
    "        modelId=\"amazon.titan-embed-image-v1\",\n",
    "        accept=\"application/json\",\n",
    "        contentType=\"application/json\")\n",
    "    response_body = json.loads(response.get(\"body\").read())\n",
    "    return np.array([response_body.get(\"embedding\")]).astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Titan Multimodal embeddings has an image size constraint of 2048px by 2048px. We will use this function to peform a image resize if needed before we send it to Titan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "\n",
    "MAX_IMAGE_HEIGHT: int = 2048\n",
    "MAX_IMAGE_WIDTH: int = 2048\n",
    "\n",
    "\n",
    "def resizeandGetByteData(imageFile):\n",
    "    image = Image.open(imageFile)\n",
    "    if (image.size[0] * image.size[1]) > (MAX_IMAGE_HEIGHT * MAX_IMAGE_WIDTH):\n",
    "        image = image.resize((MAX_IMAGE_HEIGHT, MAX_IMAGE_WIDTH))\n",
    "    with BytesIO() as output:\n",
    "        image.save(output, 'png')\n",
    "        bytes_data = output.getvalue()\n",
    "    return bytes_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function orchestrates reading the file from disk, base64 encoding the bytes then calling the functions above for sending to Titan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import os\n",
    "\n",
    "\n",
    "# enumerate over classified documents,\n",
    "# create embeddings of each and store in vector DB\n",
    "def getDocumentsandIndex(directory, classID):\n",
    "    for fileName in os.listdir(directory):\n",
    "        doc = f\"{directory}/{fileName}\"\n",
    "        if os.path.isfile(doc):\n",
    "            with open(doc, 'rb') as f:\n",
    "                if (fileName.endswith('.png') or\n",
    "                        fileName.endswith('.jpeg') or\n",
    "                        fileName.endswith('.jpg') or\n",
    "                        fileName.endswith('.tif')):\n",
    "                    bytes_data = resizeandGetByteData(f)\n",
    "                    input_image_base64 = base64.b64encode(bytes_data).decode('utf8')\n",
    "                    embeddings = getEmbeddings(input_image_base64)\n",
    "                    print(f\"Adding file {directory}/{fileName} to Index.\")\n",
    "                    indexIDMap.add_with_ids(embeddings, classID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now enumerate over our document samples, create the embeddings and save those into our Vector DB.\n",
    "For each known document type we will store a integer as a piece of meta data associated with our document in the vector DB.\n",
    "\n",
    "0. Closing Disclosure\n",
    "1. Invoices\n",
    "2. Social Security Cards\n",
    "3. W4\n",
    "4. Bank Statement\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"> \n",
    "    <b>NOTE:</b> To execute this notebook with your own sample documents, simply create a folder for each document type under sampleGallery and copy your documents into their respective folder. Modify the DOC_CLASSES array with your additional document classes and add the additional method calls to getDocumentsandIndex with your newly created folder and class.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOC_CLASSES: list[str] = [\"Closing Disclosure\", \"Invoices\", \"Social Security Card\", \"W4\", \"Bank Statement\", \"Email\"]\n",
    "\n",
    "if indexIDMap.ntotal == 0:  # populate our Vector DB if it is empty on first run\n",
    "    getDocumentsandIndex(f\"{EXTRACTDIR}/sampleGallery/ClosingDisclosure\", DOC_CLASSES.index(\"Closing Disclosure\"))\n",
    "    getDocumentsandIndex(f\"{EXTRACTDIR}/sampleGallery/Invoices\", DOC_CLASSES.index(\"Invoices\"))\n",
    "    getDocumentsandIndex(f\"{EXTRACTDIR}/sampleGallery/SSCards\", DOC_CLASSES.index(\"Social Security Card\"))\n",
    "    getDocumentsandIndex(f\"{EXTRACTDIR}/sampleGallery/W4\", DOC_CLASSES.index(\"W4\"))\n",
    "    getDocumentsandIndex(f\"{EXTRACTDIR}/sampleGallery/BankStatements\", DOC_CLASSES.index(\"Bank Statement\"))\n",
    "\n",
    "print(f\"A total of {indexIDMap.ntotal} image embeddings are stored in the vector DB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets save the FAISS Vector DB to disk. If we rerun this notebook, the DB will be read from disk and reused. If you would like to create a new Vector DB, either delete this one from disk or modify the DB name \"faissGallery.index\" found in a cell near the beginning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save our in-memory index to disk.\n",
    "faiss.write_index(indexIDMap, INDEX_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Test by doing a similarity search in the vector DB.\n",
    "Now that we have our vector DB populated with embeddings from the sample document images, our virtual gallery is ready. We will now enumerate over some additional sample documents in our testing folder, for each we will create embeddings and then we will do a similarity search against our vector DB to find the single closest match and display the result.\n",
    "You will notice the Euclidiean distance is commented out below. This value represents how close the similarity seach is, with a 0 indicating an exact match. This value could be leveraged so that should it exceed a certain threshold, the image could be surfaced to a human for verification. Uncomment to see this distance value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testingDirectory = f\"{EXTRACTDIR}/testGallery\"\n",
    "for fileName in os.listdir(testingDirectory):\n",
    "    if os.path.isfile(f\"{testingDirectory}/{fileName}\"):\n",
    "        with open(f\"{testingDirectory}/{fileName}\", \"rb\") as f:\n",
    "            if fileName.endswith('.png') or fileName.endswith('.jpeg') or fileName.endswith('.jpg') or fileName.endswith('.tif'):\n",
    "                bytes_data = resizeandGetByteData(f)\n",
    "                input_image_base64 = base64.b64encode(bytes_data).decode('utf8') \n",
    "                embeddings = getEmbeddings(input_image_base64)\n",
    "                distances = indexIDMap.search(embeddings, k=1)\n",
    "                print(f\"File Name:- {fileName} ---- Document Class:- {DOC_CLASSES[distances[1][0][0]]}\")\n",
    "                # See the distance between the search image and the retrieved image\n",
    "                print (f\"-- Vector Euclidean Distance L2 :- {distances[0][0][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from the results above, the images were correctly identified with the exeception of Email. No email image was initially included in our Vector DB gallery, so the closest match found was an Invoice document. We can improve our results going forward by adding the embeddings of an email image to our gallery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getDocumentsandIndex(f\"{EXTRACTDIR}/sampleGallery/Emails\", DOC_CLASSES.index(\"Email\"))\n",
    "print(f\"A total of {indexIDMap.ntotal} image embeddings are stored in the vector DB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have added an email image to our DB, lets rerun our tests again.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testingDirectory = f\"{EXTRACTDIR}/testGallery\"\n",
    "for fileName in os.listdir(testingDirectory):\n",
    "    if os.path.isfile(f\"{testingDirectory}/{fileName}\"):\n",
    "        with open(f\"{testingDirectory}/{fileName}\", \"rb\") as f:\n",
    "            if fileName.endswith('.png') or fileName.endswith('.jpeg') or fileName.endswith('.jpg') or fileName.endswith('.tif'):\n",
    "                bytes_data = resizeandGetByteData(f)\n",
    "                input_image_base64 = base64.b64encode(bytes_data).decode('utf8') \n",
    "                embeddings = getEmbeddings(input_image_base64)\n",
    "                distances = indexIDMap.search(embeddings, k=1)\n",
    "                print(f\"File Name:- {fileName} ---- Document Class:- {DOC_CLASSES[distances[1][0][0]]}\")\n",
    "                # If you would like to see Vector Euclidean L2 distance between\n",
    "                # image document and what was found in the gallery,\n",
    "                # uncomment next line\n",
    "                # print (f\"-- Vector Euclidean Distance L2 :- {distances[0][0][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Clean Up\n",
    "This Notebook does not create any resources or S3 buckets for cleanup. If you are running this Notebook from a newly created SageMaker’s Jupyter Notebook environment, you can stop the instance to avoid any reoccurring charges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "As demonstrated here, by using Amazon Titan multimodal embeddings we are able to create a gallery of images that we can then use for similarity search to help identify the type of document we have in hand. One of the advantages of using a vector DB containing embeddings is that we can quickly add a new set of embeddings from new documents to our gallery in near real-time to further refine our similarity search.\n",
    "\n",
    "During our testing we found this solution works very well when documents have certain unique characteristics. For example, invoices that might be processed that are received from various vendors. Each vendors invoice will typically have a certain look and feel to it like the way the line items are laid out, or having the vendor logo somewhere on the invoice. Image embeddings will achieve good results with this type of vendor classification. Where results might be more challenging is when we have documents that are dense with text from top to bottom. In this scenario it might be better to use a NLP such as Amazon Comprehend to classify on text or use a few shot prompt with an LLM."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
